{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPyluvyuXFjtbQxLcUWtcrS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "aa89ee1fd51643c6828c91a8f0e15bf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ec23e1aead404e72a94392f2afa3079f",
              "IPY_MODEL_d7532b517f66494794d1311a34a4ef88",
              "IPY_MODEL_0d442461d137433d82b26eefdbb7b357"
            ],
            "layout": "IPY_MODEL_d4be94df60d44b40ad069c88fcb579e3"
          }
        },
        "ec23e1aead404e72a94392f2afa3079f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2846e6b12424c65a6f6fc0701bdd775",
            "placeholder": "​",
            "style": "IPY_MODEL_62fe5343edf24f1b9d97df44d9858962",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "d7532b517f66494794d1311a34a4ef88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_864d8d2be28b4e96a2be48f24e5a5e5f",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8535e96581ca4c489c58b2108df88022",
            "value": 48
          }
        },
        "0d442461d137433d82b26eefdbb7b357": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce31c36632f04bc7a32337fb0e664548",
            "placeholder": "​",
            "style": "IPY_MODEL_4c055d4f7715424ead46d8078b4308da",
            "value": " 48.0/48.0 [00:00&lt;00:00, 1.34kB/s]"
          }
        },
        "d4be94df60d44b40ad069c88fcb579e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2846e6b12424c65a6f6fc0701bdd775": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62fe5343edf24f1b9d97df44d9858962": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "864d8d2be28b4e96a2be48f24e5a5e5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8535e96581ca4c489c58b2108df88022": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ce31c36632f04bc7a32337fb0e664548": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c055d4f7715424ead46d8078b4308da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "04518c71c5b9449a986bff85be3fe780": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1dd46dc7748c4100bc3714ab2313ae8c",
              "IPY_MODEL_42caf2f5339346d18a8be2ed9a19c9a6",
              "IPY_MODEL_39c6401945fa4ffbb1ce57e46747e7ac"
            ],
            "layout": "IPY_MODEL_19771a3833c5437c934da8b57d4abc05"
          }
        },
        "1dd46dc7748c4100bc3714ab2313ae8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69c6e506c5564c11bb8cb77b842172c4",
            "placeholder": "​",
            "style": "IPY_MODEL_b32b75c584884ab887788275e9e8781b",
            "value": "vocab.txt: 100%"
          }
        },
        "42caf2f5339346d18a8be2ed9a19c9a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54becb34de89467fb8e027c2864b63e6",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5e8d1cc98bb443e780e69a079e1c5047",
            "value": 231508
          }
        },
        "39c6401945fa4ffbb1ce57e46747e7ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5c09f41f38f48548b76b91115f1966b",
            "placeholder": "​",
            "style": "IPY_MODEL_14c1aedaabdd49a3abdef2eeb61238f6",
            "value": " 232k/232k [00:00&lt;00:00, 1.31MB/s]"
          }
        },
        "19771a3833c5437c934da8b57d4abc05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69c6e506c5564c11bb8cb77b842172c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b32b75c584884ab887788275e9e8781b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "54becb34de89467fb8e027c2864b63e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e8d1cc98bb443e780e69a079e1c5047": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a5c09f41f38f48548b76b91115f1966b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14c1aedaabdd49a3abdef2eeb61238f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d86d032470f348129f1dc24cc30a0ebf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6ce4a49c4af9455b891ad53d6cb37224",
              "IPY_MODEL_6cd50d5b03214f56993af33fec95e8a2",
              "IPY_MODEL_35f817736dc848e49c62e1c5bd802cf6"
            ],
            "layout": "IPY_MODEL_5db4e7ead95c40f2bcc4265eb6b67b73"
          }
        },
        "6ce4a49c4af9455b891ad53d6cb37224": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fdd599444fb04a539ceb07bc1e6d6b59",
            "placeholder": "​",
            "style": "IPY_MODEL_6968ee7c21d841a5afc5ac47e6cd60f3",
            "value": "tokenizer.json: 100%"
          }
        },
        "6cd50d5b03214f56993af33fec95e8a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5967e89cc5da4ae5ab7e7bfcca9d9c94",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1d9df4a78ef4436cb204ef4ead1e1b74",
            "value": 466062
          }
        },
        "35f817736dc848e49c62e1c5bd802cf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f56404b8f0b4b50ba921911638d18d1",
            "placeholder": "​",
            "style": "IPY_MODEL_e1c2caf7f6e1463088eeb590199e01af",
            "value": " 466k/466k [00:00&lt;00:00, 827kB/s]"
          }
        },
        "5db4e7ead95c40f2bcc4265eb6b67b73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fdd599444fb04a539ceb07bc1e6d6b59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6968ee7c21d841a5afc5ac47e6cd60f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5967e89cc5da4ae5ab7e7bfcca9d9c94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d9df4a78ef4436cb204ef4ead1e1b74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6f56404b8f0b4b50ba921911638d18d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1c2caf7f6e1463088eeb590199e01af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4a26106b54e04426bbd1f601874d4921": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2ef37229f6974cfba8d3aa0f8c285684",
              "IPY_MODEL_2b6e5bf818f44972b4cac32756690d3b",
              "IPY_MODEL_828b6519caa642cd977e6116af6ef50c"
            ],
            "layout": "IPY_MODEL_7b8998eb4fc84b338cdee4c7e9f46703"
          }
        },
        "2ef37229f6974cfba8d3aa0f8c285684": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d4f8b7e1e0b4619b06ff340b53ccc62",
            "placeholder": "​",
            "style": "IPY_MODEL_81ccd8938acb44d79991f32470c70482",
            "value": "config.json: 100%"
          }
        },
        "2b6e5bf818f44972b4cac32756690d3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aff2f7545bae4d05ac92c885195bae0b",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a2712f349da84e7ba327271718d720f1",
            "value": 570
          }
        },
        "828b6519caa642cd977e6116af6ef50c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77a9b0212d51420bbe99ba455a79c657",
            "placeholder": "​",
            "style": "IPY_MODEL_7be4938cd6f5434ca219bed912543b88",
            "value": " 570/570 [00:00&lt;00:00, 13.9kB/s]"
          }
        },
        "7b8998eb4fc84b338cdee4c7e9f46703": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d4f8b7e1e0b4619b06ff340b53ccc62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81ccd8938acb44d79991f32470c70482": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aff2f7545bae4d05ac92c885195bae0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2712f349da84e7ba327271718d720f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "77a9b0212d51420bbe99ba455a79c657": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7be4938cd6f5434ca219bed912543b88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4e712c073ac94641be86494bfc5f0bbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c3b34dc737a8486c8355129a056e30a7",
              "IPY_MODEL_4ea1d6c6c85147d2b55f4e5127cda694",
              "IPY_MODEL_0b9d5dc226d74684a0953ecefeb92474"
            ],
            "layout": "IPY_MODEL_5790002d23124072bcf8aeb4536d07b8"
          }
        },
        "c3b34dc737a8486c8355129a056e30a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cddd4ef4cefe434fbebdc68cfec2daef",
            "placeholder": "​",
            "style": "IPY_MODEL_42e249bc24b949edbd9bb0d0596eff17",
            "value": "model.safetensors: 100%"
          }
        },
        "4ea1d6c6c85147d2b55f4e5127cda694": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34cead52c40c4e31a9a2da949449dee1",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c64178fef1d34789a51685c03219eaeb",
            "value": 440449768
          }
        },
        "0b9d5dc226d74684a0953ecefeb92474": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f11d953132e477e8b4400f3475362cd",
            "placeholder": "​",
            "style": "IPY_MODEL_08b8fb60e8814778972f79011d243ded",
            "value": " 440M/440M [00:06&lt;00:00, 111MB/s]"
          }
        },
        "5790002d23124072bcf8aeb4536d07b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cddd4ef4cefe434fbebdc68cfec2daef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42e249bc24b949edbd9bb0d0596eff17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "34cead52c40c4e31a9a2da949449dee1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c64178fef1d34789a51685c03219eaeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5f11d953132e477e8b4400f3475362cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08b8fb60e8814778972f79011d243ded": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hit2421/-datascience-projects-/blob/main/FineTunning_TTS_for_English.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fvxog3eqE5_V",
        "outputId": "2bf4f335-db93-4299-9bab-66177721b08b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (0.12.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile) (2.22)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.15.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.1)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Downloading datasets-3.0.1-py3-none-any.whl (471 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-3.0.1 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch transformers datasets soundfile"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKlNrZePFMzK",
        "outputId": "64723cf0-3688-4c48-e5ff-c59f095986b0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "import soundfile as sf\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create the recordings directory if it doesn't exist\n",
        "recordings_dir = \"/content/drive/My Drive/recordings\"\n",
        "if not os.path.exists(recordings_dir):\n",
        "    os.makedirs(recordings_dir)\n",
        "    print(f\"Created directory: {recordings_dir}\")\n",
        "else:\n",
        "    print(f\"Directory already exists: {recordings_dir}\")\n",
        "\n",
        "# Load your dataset\n",
        "csv_file_path = \"/content/data.csv\"\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Initialize the SpeechT5 synthesizer\n",
        "synthesiser = pipeline(\"text-to-speech\", \"microsoft/speecht5_tts\")\n",
        "\n",
        "# Load speaker embeddings\n",
        "embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
        "speaker_embedding = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n",
        "\n",
        "# Iterate over each row in the dataframe and generate speech\n",
        "for index, row in df.iterrows():\n",
        "    text = row['text']\n",
        "    speech = synthesiser(text, forward_params={\"speaker_embeddings\": speaker_embedding})\n",
        "\n",
        "    # Save the audio file to the recordings directory\n",
        "    audio_file_path = os.path.join(recordings_dir, f\"synthesized_audio_{index}.wav\")\n",
        "    try:\n",
        "        sf.write(audio_file_path, speech[\"audio\"], samplerate=speech[\"sampling_rate\"], format='WAV')\n",
        "        print(f\"Audio saved to: {audio_file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving audio to {audio_file_path}: {e}\")\n",
        "\n",
        "print(f\"Generated audio for text: {text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avEkqayvJCb9",
        "outputId": "2d11ed0f-a7e3-4137-e19f-947565bb9937"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Created directory: /content/drive/My Drive/recordings\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Audio saved to: /content/drive/My Drive/recordings/synthesized_audio_0.wav\n",
            "Audio saved to: /content/drive/My Drive/recordings/synthesized_audio_1.wav\n",
            "Audio saved to: /content/drive/My Drive/recordings/synthesized_audio_2.wav\n",
            "Audio saved to: /content/drive/My Drive/recordings/synthesized_audio_3.wav\n",
            "Audio saved to: /content/drive/My Drive/recordings/synthesized_audio_4.wav\n",
            "Audio saved to: /content/drive/My Drive/recordings/synthesized_audio_5.wav\n",
            "Audio saved to: /content/drive/My Drive/recordings/synthesized_audio_6.wav\n",
            "Audio saved to: /content/drive/My Drive/recordings/synthesized_audio_7.wav\n",
            "Audio saved to: /content/drive/My Drive/recordings/synthesized_audio_8.wav\n",
            "Audio saved to: /content/drive/My Drive/recordings/synthesized_audio_9.wav\n",
            "Audio saved to: /content/drive/My Drive/recordings/synthesized_audio_10.wav\n",
            "Audio saved to: /content/drive/My Drive/recordings/synthesized_audio_11.wav\n",
            "Audio saved to: /content/drive/My Drive/recordings/synthesized_audio_12.wav\n",
            "Audio saved to: /content/drive/My Drive/recordings/synthesized_audio_13.wav\n",
            "Audio saved to: /content/drive/My Drive/recordings/synthesized_audio_14.wav\n",
            "Audio saved to: /content/drive/My Drive/recordings/synthesized_audio_15.wav\n",
            "Audio saved to: /content/drive/My Drive/recordings/synthesized_audio_16.wav\n",
            "Audio saved to: /content/drive/My Drive/recordings/synthesized_audio_17.wav\n",
            "Audio saved to: /content/drive/My Drive/recordings/synthesized_audio_18.wav\n",
            "Audio saved to: /content/drive/My Drive/recordings/synthesized_audio_19.wav\n",
            "Audio saved to: /content/drive/My Drive/recordings/synthesized_audio_20.wav\n",
            "Audio saved to: /content/drive/My Drive/recordings/synthesized_audio_21.wav\n",
            "Audio saved to: /content/drive/My Drive/recordings/synthesized_audio_22.wav\n",
            "Audio saved to: /content/drive/My Drive/recordings/synthesized_audio_23.wav\n",
            "Audio saved to: /content/drive/My Drive/recordings/synthesized_audio_24.wav\n",
            "Audio saved to: /content/drive/My Drive/recordings/synthesized_audio_25.wav\n",
            "Audio saved to: /content/drive/My Drive/recordings/synthesized_audio_26.wav\n",
            "Audio saved to: /content/drive/My Drive/recordings/synthesized_audio_27.wav\n",
            "Audio saved to: /content/drive/My Drive/recordings/synthesized_audio_28.wav\n",
            "Audio saved to: /content/drive/My Drive/recordings/synthesized_audio_29.wav\n",
            "Generated audio for text: How do you handle disagreements within a team?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Load your dataset\n",
        "csv_file_path = \"/content/data.csv\"  # Your CSV file path\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Initialize the processor and model for fine-tuning\n",
        "processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
        "model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\")\n",
        "\n",
        "# Load speaker embeddings\n",
        "from datasets import load_dataset\n",
        "embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
        "speaker_embedding = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n",
        "\n",
        "# Define a custom dataset class\n",
        "class TTSDataset(Dataset):\n",
        "    def __init__(self, dataframe, processor, speaker_embedding):\n",
        "        self.dataframe = dataframe\n",
        "        self.processor = processor\n",
        "        self.speaker_embedding = speaker_embedding\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.dataframe.iloc[idx]['text']\n",
        "        inputs = self.processor(text=text, return_tensors=\"pt\")\n",
        "\n",
        "        # Return tokenized text and speaker embedding\n",
        "        return {\n",
        "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
        "            \"speaker_embedding\": self.speaker_embedding\n",
        "        }\n",
        "# Create dataset and dataloader\n",
        "train_dataset = TTSDataset(df, processor, speaker_embedding)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBqmfmkbN9u5",
        "outputId": "5f7cadfb-6237-4b3b-ed4f-81c56fe5a65c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define your model\n",
        "class YourModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(YourModel, self).__init__()\n",
        "        # Define layers; adjust as necessary for your architecture\n",
        "        self.linear = nn.Linear(512, 256)  # Adjust input and output dimensions\n",
        "\n",
        "    def forward(self, input_ids, speaker_embeddings):\n",
        "        # Debug: Print shapes of inputs\n",
        "        print(f\"Input IDs shape: {input_ids.shape}, type: {type(input_ids)}\")\n",
        "        print(f\"Speaker Embeddings shape: {speaker_embeddings.shape}, type: {type(speaker_embeddings)}\")\n",
        "\n",
        "        # Process speaker embeddings to ensure correct shape\n",
        "        try:\n",
        "            output = self.linear(speaker_embeddings.squeeze(1))  # Squeeze to remove the dimension\n",
        "            print(f\"Output shape before return: {output.shape}, type: {type(output)}\")\n",
        "            return output\n",
        "        except Exception as e:\n",
        "            print(f\"Error during forward pass: {e}\")\n",
        "            raise\n",
        "\n",
        "# Instantiate the model\n",
        "model = YourModel()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_function = nn.MSELoss()  # Change as per your requirement\n",
        "\n",
        "# Example dataset (input_ids and speaker_embeddings)\n",
        "input_ids = torch.randint(0, 100, (4, 84))  # Example input IDs (batch_size, sequence_length)\n",
        "speaker_embedding = torch.randn(4, 1, 512)\n",
        "\n",
        "try:\n",
        "    dummy_output = model(input_ids, speaker_embedding)\n",
        "    print(f\"Dummy output shape: {dummy_output.shape}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during dummy forward pass: {e}\")\n",
        "\n",
        "# Training loop (example)\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    try:\n",
        "        # Forward pass\n",
        "        speech_outputs = model(input_ids=input_ids, speaker_embeddings=speaker_embedding)\n",
        "\n",
        "        # Define your target here, for example:\n",
        "        ground_truth_audio = torch.randn(4, 256)  # Example ground truth (adjust as needed)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_function(speech_outputs, ground_truth_audio)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during training epoch {epoch + 1}: {e}\")\n",
        "        break  # Exit the loop to avoid further errors\n",
        "\n",
        "# Save the fine-tuned model\n",
        "torch.save(model.state_dict(), 'fine_tuned_model.pth')\n",
        "print(\"Model fine-tuned and saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rH3u23WqOWT2",
        "outputId": "b8af779d-d237-4218-a77e-e26602267857"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDs shape: torch.Size([4, 84]), type: <class 'torch.Tensor'>\n",
            "Speaker Embeddings shape: torch.Size([4, 1, 512]), type: <class 'torch.Tensor'>\n",
            "Output shape before return: torch.Size([4, 256]), type: <class 'torch.Tensor'>\n",
            "Dummy output shape: torch.Size([4, 256])\n",
            "Input IDs shape: torch.Size([4, 84]), type: <class 'torch.Tensor'>\n",
            "Speaker Embeddings shape: torch.Size([4, 1, 512]), type: <class 'torch.Tensor'>\n",
            "Output shape before return: torch.Size([4, 256]), type: <class 'torch.Tensor'>\n",
            "Epoch 1/10, Loss: 1.3854542970657349\n",
            "Input IDs shape: torch.Size([4, 84]), type: <class 'torch.Tensor'>\n",
            "Speaker Embeddings shape: torch.Size([4, 1, 512]), type: <class 'torch.Tensor'>\n",
            "Output shape before return: torch.Size([4, 256]), type: <class 'torch.Tensor'>\n",
            "Epoch 2/10, Loss: 1.1743329763412476\n",
            "Input IDs shape: torch.Size([4, 84]), type: <class 'torch.Tensor'>\n",
            "Speaker Embeddings shape: torch.Size([4, 1, 512]), type: <class 'torch.Tensor'>\n",
            "Output shape before return: torch.Size([4, 256]), type: <class 'torch.Tensor'>\n",
            "Epoch 3/10, Loss: 1.276724934577942\n",
            "Input IDs shape: torch.Size([4, 84]), type: <class 'torch.Tensor'>\n",
            "Speaker Embeddings shape: torch.Size([4, 1, 512]), type: <class 'torch.Tensor'>\n",
            "Output shape before return: torch.Size([4, 256]), type: <class 'torch.Tensor'>\n",
            "Epoch 4/10, Loss: 1.1176735162734985\n",
            "Input IDs shape: torch.Size([4, 84]), type: <class 'torch.Tensor'>\n",
            "Speaker Embeddings shape: torch.Size([4, 1, 512]), type: <class 'torch.Tensor'>\n",
            "Output shape before return: torch.Size([4, 256]), type: <class 'torch.Tensor'>\n",
            "Epoch 5/10, Loss: 1.158743143081665\n",
            "Input IDs shape: torch.Size([4, 84]), type: <class 'torch.Tensor'>\n",
            "Speaker Embeddings shape: torch.Size([4, 1, 512]), type: <class 'torch.Tensor'>\n",
            "Output shape before return: torch.Size([4, 256]), type: <class 'torch.Tensor'>\n",
            "Epoch 6/10, Loss: 1.1343107223510742\n",
            "Input IDs shape: torch.Size([4, 84]), type: <class 'torch.Tensor'>\n",
            "Speaker Embeddings shape: torch.Size([4, 1, 512]), type: <class 'torch.Tensor'>\n",
            "Output shape before return: torch.Size([4, 256]), type: <class 'torch.Tensor'>\n",
            "Epoch 7/10, Loss: 1.2963312864303589\n",
            "Input IDs shape: torch.Size([4, 84]), type: <class 'torch.Tensor'>\n",
            "Speaker Embeddings shape: torch.Size([4, 1, 512]), type: <class 'torch.Tensor'>\n",
            "Output shape before return: torch.Size([4, 256]), type: <class 'torch.Tensor'>\n",
            "Epoch 8/10, Loss: 1.1659115552902222\n",
            "Input IDs shape: torch.Size([4, 84]), type: <class 'torch.Tensor'>\n",
            "Speaker Embeddings shape: torch.Size([4, 1, 512]), type: <class 'torch.Tensor'>\n",
            "Output shape before return: torch.Size([4, 256]), type: <class 'torch.Tensor'>\n",
            "Epoch 9/10, Loss: 1.1521055698394775\n",
            "Input IDs shape: torch.Size([4, 84]), type: <class 'torch.Tensor'>\n",
            "Speaker Embeddings shape: torch.Size([4, 1, 512]), type: <class 'torch.Tensor'>\n",
            "Output shape before return: torch.Size([4, 256]), type: <class 'torch.Tensor'>\n",
            "Epoch 10/10, Loss: 1.1700773239135742\n",
            "Model fine-tuned and saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install g2p-en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZ1RMhbfOqzs",
        "outputId": "c0219cde-3f89-453f-9192-def598fde23d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting g2p-en\n",
            "  Downloading g2p_en-2.1.0-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: numpy>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from g2p-en) (1.26.4)\n",
            "Requirement already satisfied: nltk>=3.2.4 in /usr/local/lib/python3.10/dist-packages (from g2p-en) (3.8.1)\n",
            "Requirement already satisfied: inflect>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from g2p-en) (7.4.0)\n",
            "Collecting distance>=0.1.3 (from g2p-en)\n",
            "  Downloading Distance-0.1.3.tar.gz (180 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.3/180.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools>=8.5.0 in /usr/local/lib/python3.10/dist-packages (from inflect>=0.3.1->g2p-en) (10.5.0)\n",
            "Requirement already satisfied: typeguard>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from inflect>=0.3.1->g2p-en) (4.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.4->g2p-en) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.4->g2p-en) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.4->g2p-en) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.4->g2p-en) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from typeguard>=4.0.1->inflect>=0.3.1->g2p-en) (4.12.2)\n",
            "Downloading g2p_en-2.1.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: distance\n",
            "  Building wheel for distance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for distance: filename=Distance-0.1.3-py3-none-any.whl size=16256 sha256=b186aad39fbc7166e60da3668bda3c331725c877f1f5e077943434745c732e8a\n",
            "  Stored in directory: /root/.cache/pip/wheels/e8/bb/de/f71bf63559ea9a921059a5405806f7ff6ed612a9231c4a9309\n",
            "Successfully built distance\n",
            "Installing collected packages: distance, g2p-en\n",
            "Successfully installed distance-0.1.3 g2p-en-2.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchaudio\n",
        "from g2p_en import G2p\n",
        "\n",
        "# Assuming you have defined your TTS model\n",
        "class YourModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(YourModel, self).__init__()\n",
        "        # Define layers\n",
        "        self.linear = nn.Linear(512, 256)  # Example layer, adjust as necessary\n",
        "\n",
        "    def forward(self, input_ids, speaker_embeddings):\n",
        "        # Debug: Print shapes of inputs\n",
        "        print(f\"Input IDs shape: {input_ids.shape}, type: {type(input_ids)}\")\n",
        "        print(f\"Speaker Embeddings shape: {speaker_embeddings.shape}, type: {type(speaker_embeddings)}\")\n",
        "\n",
        "        # Forward pass through the model\n",
        "        output = self.linear(speaker_embeddings.squeeze(1))  # Ensure the shape is correct\n",
        "        print(f\"Output shape before return: {output.shape}, type: {type(output)}\")\n",
        "        return output\n",
        "\n",
        "# Instantiate the model\n",
        "model = YourModel()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_function = nn.MSELoss()  # Change as per your requirement\n",
        "\n",
        "# Function to preprocess input sentences\n",
        "def preprocess_input(sentences):\n",
        "    # Placeholder function: Convert sentences to input_ids and speaker_embeddings\n",
        "    # Here, you can include your actual preprocessing logic\n",
        "    input_ids = torch.randint(0, 100, (len(sentences), 84))  # Example input IDs\n",
        "    speaker_embeddings = torch.randn(len(sentences), 1, 512)  # Example speaker embeddings\n",
        "    return input_ids, speaker_embeddings\n",
        "\n",
        "# Function to generate speech output\n",
        "def generate_speech_output(model, sentences):\n",
        "    input_ids, speaker_embeddings = preprocess_input(sentences)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()  # Set the model to evaluation mode\n",
        "        outputs = model(input_ids, speaker_embeddings)\n",
        "\n",
        "    return outputs  # Return the generated audio\n",
        "\n",
        "# Function to save generated audio\n",
        "def save_audio(tensor_output, filename):\n",
        "    # Convert output tensor to audio and save\n",
        "    torchaudio.save(filename, tensor_output.unsqueeze(0), sample_rate=22050)  # Adjust sample rate as necessary\n",
        "\n",
        "# Function to get phonemes\n",
        "g2p = G2p()\n",
        "\n",
        "def get_phonemes(sentence):\n",
        "    return g2p(sentence)\n",
        "\n",
        "# Prepare test sentences with technical terms\n",
        "test_sentences = [\n",
        "    \"Can you explain how an API works?\",\n",
        "    \"What is the purpose of CUDA in parallel computing?\",\n",
        "    \"Describe the importance of version control systems.\",\n",
        "    \"How do you handle error handling in your code?\",\n",
        "    \"Explain the difference between HTTP and HTTPS.\"\n",
        "]\n",
        "\n",
        "# Generate speech outputs for the test sentences\n",
        "audio_outputs = generate_speech_output(model, test_sentences)\n",
        "\n",
        "# Save each audio output for evaluation\n",
        "for idx, sentence in enumerate(test_sentences):\n",
        "    save_audio(audio_outputs[idx], f\"output_{idx}.wav\")\n",
        "\n",
        "# Print phoneme representations for further analysis\n",
        "phoneme_representations = [get_phonemes(sentence) for sentence in test_sentences]\n",
        "for sentence, phoneme in zip(test_sentences, phoneme_representations):\n",
        "    print(f\"Sentence: {sentence} -> Phonemes: {phoneme}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zinpFG3UOyVD",
        "outputId": "7b7dff46-d4a4-49e0-8f47-1f223d2f11f8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDs shape: torch.Size([5, 84]), type: <class 'torch.Tensor'>\n",
            "Speaker Embeddings shape: torch.Size([5, 1, 512]), type: <class 'torch.Tensor'>\n",
            "Output shape before return: torch.Size([5, 256]), type: <class 'torch.Tensor'>\n",
            "Sentence: Can you explain how an API works? -> Phonemes: ['K', 'AE1', 'N', ' ', 'Y', 'UW1', ' ', 'IH0', 'K', 'S', 'P', 'L', 'EY1', 'N', ' ', 'HH', 'AW1', ' ', 'AE1', 'N', ' ', 'AA1', 'P', 'IY0', ' ', 'W', 'ER1', 'K', 'S', ' ', '?']\n",
            "Sentence: What is the purpose of CUDA in parallel computing? -> Phonemes: ['W', 'AH1', 'T', ' ', 'IH1', 'Z', ' ', 'DH', 'AH0', ' ', 'P', 'ER1', 'P', 'AH0', 'S', ' ', 'AH1', 'V', ' ', 'K', 'Y', 'UW1', 'D', 'AH0', ' ', 'IH0', 'N', ' ', 'P', 'EH1', 'R', 'AH0', 'L', 'EH2', 'L', ' ', 'K', 'AH0', 'M', 'P', 'Y', 'UW1', 'T', 'IH0', 'NG', ' ', '?']\n",
            "Sentence: Describe the importance of version control systems. -> Phonemes: ['D', 'IH0', 'S', 'K', 'R', 'AY1', 'B', ' ', 'DH', 'AH0', ' ', 'IH0', 'M', 'P', 'AO1', 'R', 'T', 'AH0', 'N', 'S', ' ', 'AH1', 'V', ' ', 'V', 'ER1', 'ZH', 'AH0', 'N', ' ', 'K', 'AH0', 'N', 'T', 'R', 'OW1', 'L', ' ', 'S', 'IH1', 'S', 'T', 'AH0', 'M', 'Z', ' ', '.']\n",
            "Sentence: How do you handle error handling in your code? -> Phonemes: ['HH', 'AW1', ' ', 'D', 'UW1', ' ', 'Y', 'UW1', ' ', 'HH', 'AE1', 'N', 'D', 'AH0', 'L', ' ', 'EH1', 'R', 'ER0', ' ', 'HH', 'AE1', 'N', 'D', 'L', 'IH0', 'NG', ' ', 'IH0', 'N', ' ', 'Y', 'AO1', 'R', ' ', 'K', 'OW1', 'D', ' ', '?']\n",
            "Sentence: Explain the difference between HTTP and HTTPS. -> Phonemes: ['IH0', 'K', 'S', 'P', 'L', 'EY1', 'N', ' ', 'DH', 'AH0', ' ', 'D', 'IH1', 'F', 'ER0', 'AH0', 'N', 'S', ' ', 'B', 'IH0', 'T', 'W', 'IY1', 'N', ' ', 'T', 'AE1', 'P', 'T', 'IY1', ' ', 'AH0', 'N', 'D', ' ', 'T', 'AE1', 'P', 'T', 'S', ' ', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDAdkyiZPFQS",
        "outputId": "6921d384-2b7b-4123-be22-b6ab0f2b7433"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "import soundfile as sf  # For saving audio\n",
        "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech\n",
        "from torch.quantization import quantize_dynamic\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# Step 1: Load the Original Model and Processor\n",
        "# -------------------------------------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
        "model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\").to(device)\n",
        "\n",
        "print(\"Model and processor loaded.\")\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# Step 2: Model Quantization\n",
        "# -------------------------------------------------------\n",
        "print(\"Starting model quantization...\")\n",
        "quantized_model = quantize_dynamic(\n",
        "    model,  # Model to quantize\n",
        "    {torch.nn.Linear},  # Quantize only Linear layers\n",
        "    dtype=torch.qint8  # Use 8-bit integers\n",
        ").to(device)\n",
        "print(\"Quantized model ready.\")\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# Step 3: Model Pruning (Optional, Simulated Here)\n",
        "# -------------------------------------------------------\n",
        "print(\"Starting model pruning...\")\n",
        "\n",
        "# Simulate pruning by removing unused layers (or setting weights to zero)\n",
        "for name, param in quantized_model.named_parameters():\n",
        "    if \"weight\" in name:\n",
        "        param.data *= (torch.rand_like(param.data) > 0.2).float()\n",
        "\n",
        "print(\"Model pruning complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhnQpYmiPTZr",
        "outputId": "cb3bae5b-5a3d-4a95-d614-2df881f8b9e8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and processor loaded.\n",
            "Starting model quantization...\n",
            "Quantized model ready.\n",
            "Starting model pruning...\n",
            "Model pruning complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sounddevice"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtIHvr_qPbxY",
        "outputId": "8de242c0-ad3c-4124-c80a-eaee52eaf6ba"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sounddevice in /usr/local/lib/python3.10/dist-packages (0.5.1)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice) (2.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update  # Update package lists\n",
        "!apt-get install libportaudio2  # Install the PortAudio library"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24otRn2QPgKm",
        "outputId": "e2b0a67c-77e7-4d6a-aae3-0d84ec6b5c1a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,032 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Ign:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:9 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,450 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,596 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,648 kB]\n",
            "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,403 kB]\n",
            "Fetched 16.5 MB in 5s (3,574 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  libportaudio2\n",
            "0 upgraded, 1 newly installed, 0 to remove and 50 not upgraded.\n",
            "Need to get 65.3 kB of archives.\n",
            "After this operation, 223 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libportaudio2 amd64 19.6.0-1.1 [65.3 kB]\n",
            "Fetched 65.3 kB in 1s (69.3 kB/s)\n",
            "Selecting previously unselected package libportaudio2:amd64.\n",
            "(Reading database ... 123629 files and directories currently installed.)\n",
            "Preparing to unpack .../libportaudio2_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking libportaudio2:amd64 (19.6.0-1.1) ...\n",
            "Setting up libportaudio2:amd64 (19.6.0-1.1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sounddevice as sd"
      ],
      "metadata": {
        "id": "y8KXunM5QJCW"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hvewjo2GQR59",
        "outputId": "6e1915a6-0cbb-4b96-81a4-35e6381d18bc"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.15.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "import soundfile as sf  # For saving audio\n",
        "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech\n",
        "from torch.quantization import quantize_dynamic\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# Step 1: Load the Original Model and Processor\n",
        "# -------------------------------------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
        "model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\").to(device)\n",
        "\n",
        "print(\"Model and processor loaded.\")\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# Step 2: Model Quantization\n",
        "# -------------------------------------------------------\n",
        "print(\"Starting model quantization...\")\n",
        "quantized_model = quantize_dynamic(\n",
        "    model,  # Model to quantize\n",
        "    {torch.nn.Linear},  # Quantize only Linear layers\n",
        "    dtype=torch.qint8  # Use 8-bit integers\n",
        ").to(device)\n",
        "print(\"Quantized model ready.\")\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# Step 3: Model Pruning (Optional, Simulated Here)\n",
        "# -------------------------------------------------------\n",
        "print(\"Starting model pruning...\")\n",
        "\n",
        "# Simulate pruning by removing unused layers (or setting weights to zero)\n",
        "for name, param in quantized_model.named_parameters():\n",
        "    if \"weight\" in name:\n",
        "        param.data *= (torch.rand_like(param.data) > 0.2).float()\n",
        "\n",
        "print(\"Model pruning complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QqEbXEJQViD",
        "outputId": "11d4972a-a61f-4f57-95cc-6e09456a0f6f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and processor loaded.\n",
            "Starting model quantization...\n",
            "Quantized model ready.\n",
            "Starting model pruning...\n",
            "Model pruning complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "_kgEBFmmQgX8"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import SpeechT5Processor\n",
        "\n",
        "# Initialize the processor outside your function, if you haven't done that yet.\n",
        "processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
        "\n",
        "def generate_audio_and_measure_time(model, text, speaker_embeddings):\n",
        "    # Prepare inputs\n",
        "    inputs = processor(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        speech = model.generate(\n",
        "            **inputs,\n",
        "            speaker_embeddings=speaker_embeddings\n",
        "        )\n",
        "    end_time = time.time()\n",
        "\n",
        "    inference_time = end_time - start_time\n",
        "    return inference_time, speech"
      ],
      "metadata": {
        "id": "xTTjJ38AQlcy"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move model and inputs to the same device\n",
        "quantized_model.to(device)  # Ensure the quantized model is on the correct device\n",
        "speaker_embedding = speaker_embedding.to(device)  # Ensure speaker embeddings are also on the correct device"
      ],
      "metadata": {
        "id": "dAy1SLxLQqkQ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cpu\")  # Change to CPU if you're using a quantized model\n",
        "quantized_model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-Oz_87xQugr",
        "outputId": "30a404dd-9f75-4d7a-bd6c-4d0ada7bc612"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SpeechT5ForTextToSpeech(\n",
              "  (speecht5): SpeechT5Model(\n",
              "    (encoder): SpeechT5EncoderWithTextPrenet(\n",
              "      (prenet): SpeechT5TextEncoderPrenet(\n",
              "        (embed_tokens): Embedding(81, 768, padding_idx=1)\n",
              "        (encode_positions): SpeechT5ScaledPositionalEncoding(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (wrapped_encoder): SpeechT5Encoder(\n",
              "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (layers): ModuleList(\n",
              "          (0-11): 12 x SpeechT5EncoderLayer(\n",
              "            (attention): SpeechT5Attention(\n",
              "              (k_proj): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "              (v_proj): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "              (q_proj): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "              (out_proj): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (feed_forward): SpeechT5FeedForward(\n",
              "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
              "              (intermediate_dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "              (output_dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "        (embed_positions): SpeechT5RelativePositionalEncoding(\n",
              "          (pe_k): Embedding(320, 64)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (decoder): SpeechT5DecoderWithSpeechPrenet(\n",
              "      (prenet): SpeechT5SpeechDecoderPrenet(\n",
              "        (layers): ModuleList(\n",
              "          (0): DynamicQuantizedLinear(in_features=80, out_features=256, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "          (1): DynamicQuantizedLinear(in_features=256, out_features=256, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "        )\n",
              "        (final_layer): DynamicQuantizedLinear(in_features=256, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "        (encode_positions): SpeechT5ScaledPositionalEncoding(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (speaker_embeds_layer): DynamicQuantizedLinear(in_features=1280, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "      )\n",
              "      (wrapped_decoder): SpeechT5Decoder(\n",
              "        (layers): ModuleList(\n",
              "          (0-5): 6 x SpeechT5DecoderLayer(\n",
              "            (self_attn): SpeechT5Attention(\n",
              "              (k_proj): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "              (v_proj): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "              (q_proj): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "              (out_proj): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (encoder_attn): SpeechT5Attention(\n",
              "              (k_proj): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "              (v_proj): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "              (q_proj): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "              (out_proj): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "            )\n",
              "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (feed_forward): SpeechT5FeedForward(\n",
              "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
              "              (intermediate_dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "              (output_dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (speech_decoder_postnet): SpeechT5SpeechDecoderPostnet(\n",
              "    (feat_out): DynamicQuantizedLinear(in_features=768, out_features=160, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "    (prob_out): DynamicQuantizedLinear(in_features=768, out_features=2, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "    (layers): ModuleList(\n",
              "      (0): SpeechT5BatchNormConvLayer(\n",
              "        (conv): Conv1d(80, 256, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
              "        (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (activation): Tanh()\n",
              "        (dropout): Dropout(p=0.5, inplace=False)\n",
              "      )\n",
              "      (1-3): 3 x SpeechT5BatchNormConvLayer(\n",
              "        (conv): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
              "        (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (activation): Tanh()\n",
              "        (dropout): Dropout(p=0.5, inplace=False)\n",
              "      )\n",
              "      (4): SpeechT5BatchNormConvLayer(\n",
              "        (conv): Conv1d(256, 80, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
              "        (batch_norm): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (dropout): Dropout(p=0.5, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import soundfile as sf\n",
        "import torch\n",
        "\n",
        "# List of technical questions to test\n",
        "technical_questions = [\n",
        "    \"What is an API?\",\n",
        "    \"The API provides interfaces for different software to communicate.\",\n",
        "    \"We're implementing GraphQL to optimize our API queries\",\n",
        "    \"What are the differences between supervised and unsupervised learning?\"\n",
        "]\n",
        "\n",
        "class TextProcessor:\n",
        "    \"\"\"Text processor class.\"\"\"\n",
        "\n",
        "    def __init__(self, processor=None):\n",
        "        # Initialize the text processor here\n",
        "        self.processor = processor\n",
        "\n",
        "    def process_text(self, text):\n",
        "        \"\"\"Process the text using the configured processor.\"\"\"\n",
        "\n",
        "        if self.processor is None:\n",
        "            raise ValueError(\"Processor not initialized\")\n",
        "\n",
        "        return self.processor(text)\n",
        "\n",
        "class TTSModel: # Corrected indentation for class definition\n",
        "    \"\"\"Text-to-speech model class.\"\"\"\n",
        "\n",
        "    def __init__(self, model=None): # Corrected indentation for method definition\n",
        "        # Initialize the text-to-speech model here\n",
        "        self.model = model\n",
        "\n",
        "    def generate_audio(self, processor, text): # Corrected indentation for method definition\n",
        "        \"\"\"Generate audio using the configured model and processor.\"\"\"\n",
        "\n",
        "        if self.model is None or processor.processor is None:\n",
        "            raise ValueError(\"Model or processor not initialized\")\n",
        "\n",
        "        inputs = processor.process_text(text)\n",
        "\n",
        "        start_time = time.time()\n",
        "        with torch.no_grad():\n",
        "            # This is a mock-up for the actual model's generate method\n",
        "            speech = torch.randn(1, 16000)  # Dummy audio data\n",
        "        end_time = time.time()\n",
        "\n",
        "        inference_time = end_time - start_time\n",
        "        print(f\"Inference time for '{text}': {inference_time:.4f} seconds\")\n",
        "        # Save the dummy audio (replace this with actual audio processing in real use cases)\n",
        "        audio = speech[0].numpy()  # Mock-up decoding step\n",
        "        sf.write(f\"{text[:10]}.wav\", audio, samplerate=16000)\n",
        "        print(f\"Audio saved for: {text}\")\n",
        "\n",
        "        return inference_time\n",
        "\n",
        "# Initialize variables\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available, otherwise use CPU\n",
        "\n",
        "# Mock text processor function (replace with actual processor function in real use)\n",
        "def mock_text_processor(text):\n",
        "    return {\"input_ids\": torch.randint(0, 100, size=(1, 128)), \"attention_mask\": torch.randint(0, 2, size=(1, 128))}\n",
        "\n",
        "processor = TextProcessor(processor=mock_text_processor)\n",
        "\n",
        "# Mock TTS model (replace with actual model in real use)\n",
        "tts_model = TTSModel(model=True)  # Set to True just to avoid the error\n",
        "\n",
        "# Function to generate audio and measure inference time\n",
        "def generate_audio_and_measure_time(model, text):\n",
        "    \"\"\"Generates audio and measures inference time.\"\"\"\n",
        "    return model.generate_audio(processor, text)\n",
        "\n",
        "# List to keep track of inference times\n",
        "inference_times = []\n",
        "for question in technical_questions:\n",
        "    time_taken = generate_audio_and_measure_time(tts_model, question)\n",
        "    inference_times.append(time_taken)\n",
        "\n",
        "# Calculate the average inference time\n",
        "average_inference_time = sum(inference_times) / len(inference_times)\n",
        "print(f\"Average inference time: {average_inference_time:.4f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xj_kbIveQy43",
        "outputId": "480a462c-6f8b-462f-d4b3-cafb549b1d73"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference time for 'What is an API?': 0.0003 seconds\n",
            "Audio saved for: What is an API?\n",
            "Inference time for 'The API provides interfaces for different software to communicate.': 0.0002 seconds\n",
            "Audio saved for: The API provides interfaces for different software to communicate.\n",
            "Inference time for 'We're implementing GraphQL to optimize our API queries': 0.0002 seconds\n",
            "Audio saved for: We're implementing GraphQL to optimize our API queries\n",
            "Inference time for 'What are the differences between supervised and unsupervised learning?': 0.0002 seconds\n",
            "Audio saved for: What are the differences between supervised and unsupervised learning?\n",
            "Average inference time: 0.0003 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch.quantization as quant\n",
        "\n",
        "# Load a pre-trained BERT model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Quantize the BERT model (dynamic quantization)\n",
        "quantized_model = quant.quantize_dynamic(\n",
        "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
        ")\n",
        "\n",
        "# Tokenize your text data\n",
        "inputs = tokenizer(\"Quantization is interesting.\", return_tensors=\"pt\")\n",
        "\n",
        "# Measure inference time\n",
        "import time\n",
        "start = time.time()\n",
        "with torch.no_grad():\n",
        "    outputs = quantized_model(**inputs)\n",
        "end = time.time()\n",
        "\n",
        "print(f\"Inference Time: {end - start:.4f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249,
          "referenced_widgets": [
            "aa89ee1fd51643c6828c91a8f0e15bf0",
            "ec23e1aead404e72a94392f2afa3079f",
            "d7532b517f66494794d1311a34a4ef88",
            "0d442461d137433d82b26eefdbb7b357",
            "d4be94df60d44b40ad069c88fcb579e3",
            "e2846e6b12424c65a6f6fc0701bdd775",
            "62fe5343edf24f1b9d97df44d9858962",
            "864d8d2be28b4e96a2be48f24e5a5e5f",
            "8535e96581ca4c489c58b2108df88022",
            "ce31c36632f04bc7a32337fb0e664548",
            "4c055d4f7715424ead46d8078b4308da",
            "04518c71c5b9449a986bff85be3fe780",
            "1dd46dc7748c4100bc3714ab2313ae8c",
            "42caf2f5339346d18a8be2ed9a19c9a6",
            "39c6401945fa4ffbb1ce57e46747e7ac",
            "19771a3833c5437c934da8b57d4abc05",
            "69c6e506c5564c11bb8cb77b842172c4",
            "b32b75c584884ab887788275e9e8781b",
            "54becb34de89467fb8e027c2864b63e6",
            "5e8d1cc98bb443e780e69a079e1c5047",
            "a5c09f41f38f48548b76b91115f1966b",
            "14c1aedaabdd49a3abdef2eeb61238f6",
            "d86d032470f348129f1dc24cc30a0ebf",
            "6ce4a49c4af9455b891ad53d6cb37224",
            "6cd50d5b03214f56993af33fec95e8a2",
            "35f817736dc848e49c62e1c5bd802cf6",
            "5db4e7ead95c40f2bcc4265eb6b67b73",
            "fdd599444fb04a539ceb07bc1e6d6b59",
            "6968ee7c21d841a5afc5ac47e6cd60f3",
            "5967e89cc5da4ae5ab7e7bfcca9d9c94",
            "1d9df4a78ef4436cb204ef4ead1e1b74",
            "6f56404b8f0b4b50ba921911638d18d1",
            "e1c2caf7f6e1463088eeb590199e01af",
            "4a26106b54e04426bbd1f601874d4921",
            "2ef37229f6974cfba8d3aa0f8c285684",
            "2b6e5bf818f44972b4cac32756690d3b",
            "828b6519caa642cd977e6116af6ef50c",
            "7b8998eb4fc84b338cdee4c7e9f46703",
            "6d4f8b7e1e0b4619b06ff340b53ccc62",
            "81ccd8938acb44d79991f32470c70482",
            "aff2f7545bae4d05ac92c885195bae0b",
            "a2712f349da84e7ba327271718d720f1",
            "77a9b0212d51420bbe99ba455a79c657",
            "7be4938cd6f5434ca219bed912543b88",
            "4e712c073ac94641be86494bfc5f0bbf",
            "c3b34dc737a8486c8355129a056e30a7",
            "4ea1d6c6c85147d2b55f4e5127cda694",
            "0b9d5dc226d74684a0953ecefeb92474",
            "5790002d23124072bcf8aeb4536d07b8",
            "cddd4ef4cefe434fbebdc68cfec2daef",
            "42e249bc24b949edbd9bb0d0596eff17",
            "34cead52c40c4e31a9a2da949449dee1",
            "c64178fef1d34789a51685c03219eaeb",
            "5f11d953132e477e8b4400f3475362cd",
            "08b8fb60e8814778972f79011d243ded"
          ]
        },
        "id": "8Je4cacnRcju",
        "outputId": "759a5eb8-2cda-4135-fdaf-4d09da42c3d2"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aa89ee1fd51643c6828c91a8f0e15bf0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "04518c71c5b9449a986bff85be3fe780"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d86d032470f348129f1dc24cc30a0ebf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4a26106b54e04426bbd1f601874d4921"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4e712c073ac94641be86494bfc5f0bbf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference Time: 0.0799 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.quantization as quant\n",
        "\n",
        "# Example PyTorch model (replace with your actual model)\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.fc = nn.Linear(128, 64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Initialize the model\n",
        "torch_model = MyModel()\n",
        "\n",
        "# Quantize the model using dynamic quantization (for example)\n",
        "quantized_model = quant.quantize_dynamic(\n",
        "    torch_model,  # the model to quantize\n",
        "    {nn.Linear},  # the layers to quantize (in this case, Linear layers)\n",
        "    dtype=torch.qint8  # data type for quantization\n",
        ")\n",
        "\n",
        "# Calculate model size in MB\n",
        "def calculate_model_size(model):\n",
        "    \"\"\"Calculate the model size in MB.\"\"\"\n",
        "    param_size = sum(p.numel() for p in model.parameters()) * 4 / (1024 ** 2)  # 32-bit (4 bytes) precision\n",
        "    buffer_size = sum(b.numel() for b in model.buffers()) * 4 / (1024 ** 2)  # 32-bit (4 bytes) precision\n",
        "    total_size = param_size + buffer_size\n",
        "    return total_size\n",
        "# Print quantized model size\n",
        "model_size_mb = calculate_model_size(quantized_model)\n",
        "print(f\"Quantized Model Size: {model_size_mb:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ztj0JS1Rmhn",
        "outputId": "a6c2e0a6-31e6-44af-922c-61cccaddb1dc"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantized Model Size: 0.00 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_final_summary(model, average_inference_time):\n",
        "    \"\"\"Prints the final summary of the model and results.\"\"\"\n",
        "    if hasattr(model, 'parameters'):\n",
        "        model_size_mb = sum(p.numel() for p in model.parameters()) * 4 / (1024 ** 2)  # Size in MB\n",
        "        print(f\"Quantized Model Size: {model_size_mb:.2f} MB\")\n",
        "    else:\n",
        "        print(\"Model does not have parameters to calculate size.\")\n",
        "\n",
        "    print(f\"Average Inference Time: {average_inference_time:.4f} seconds\")\n",
        "\n",
        "# Example of calling the function\n",
        "print_final_summary(tts_model, average_inference_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rg52pxKdRvju",
        "outputId": "999352e9-e318-4994-d800-6b4c0546b266"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model does not have parameters to calculate size.\n",
            "Average Inference Time: 0.0003 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def measure_inference_time(model, inputs):\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    end_time = time.time()\n",
        "    inference_time = end_time - start_time\n",
        "    return inference_time\n",
        "\n",
        "# Measure inference time for a sample input\n",
        "inference_time_before = measure_inference_time(model, inputs)\n",
        "print(f\"Inference Time Before Quantization: {inference_time_before:.4f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfGnk4fHR0Fn",
        "outputId": "8d504cef-4355-4a27-e8ca-7e5f3909f881"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference Time Before Quantization: 0.1491 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.quantization as quant\n",
        "\n",
        "# Apply dynamic quantization to the model\n",
        "quantized_model = quant.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n",
        "\n",
        "# Measure inference time for quantized model\n",
        "inference_time_after = measure_inference_time(quantized_model, inputs)\n",
        "print(f\"Inference Time After Quantization: {inference_time_after:.4f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BX9fGZl6R4_R",
        "outputId": "ddbcf5fb-4c09-414d-8e60-6eff8ad2bda9"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference Time After Quantization: 0.0479 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Placeholder function for quantization (adapt as necessary)\n",
        "def quantize_model(model):\n",
        "    # Perform quantization logic here\n",
        "    # This is just a placeholder, implement quantization based on your model\n",
        "    return model  # Return the quantized model\n",
        "\n",
        "# Quantize the synthesizer model\n",
        "quantized_synthesiser = quantize_model(synthesiser)  # Update this with the actual quantization method"
      ],
      "metadata": {
        "id": "3XDxn_P6Up-i"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5S7_zzCQfKhw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}